{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc790a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.57.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.8.11)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.20.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.11.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.13.2-cp313-cp313-win_amd64.whl (452 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.0 MB 2.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 1.3/28.0 MB 3.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.6/28.0 MB 4.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.9/28.0 MB 5.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.7/28.0 MB 5.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 5.2/28.0 MB 4.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 6.8/28.0 MB 4.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 8.4/28.0 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 9.7/28.0 MB 5.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 10.2/28.0 MB 5.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 11.0/28.0 MB 4.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 12.3/28.0 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 13.9/28.0 MB 5.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 14.4/28.0 MB 5.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 15.2/28.0 MB 5.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 15.7/28.0 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 17.0/28.0 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 18.6/28.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 19.9/28.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 20.7/28.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.5/28.0 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 23.1/28.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 24.6/28.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.0/28.0 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.7/28.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 5.1 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, sentencepiece, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\n",
      "   -- -------------------------------------  1/14 [sentencepiece]\n",
      "  Attempting uninstall: pyarrow\n",
      "   -- -------------------------------------  1/14 [sentencepiece]\n",
      "    Found existing installation: pyarrow 20.0.0\n",
      "   -- -------------------------------------  1/14 [sentencepiece]\n",
      "    Uninstalling pyarrow-20.0.0:\n",
      "   -- -------------------------------------  1/14 [sentencepiece]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "      Successfully uninstalled pyarrow-20.0.0\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----- ----------------------------------  2/14 [pyarrow]\n",
      "   ----------- ----------------------------  4/14 [multidict]\n",
      "  Attempting uninstall: fsspec\n",
      "   ----------- ----------------------------  4/14 [multidict]\n",
      "    Found existing installation: fsspec 2025.12.0\n",
      "   ----------- ----------------------------  4/14 [multidict]\n",
      "    Uninstalling fsspec-2025.12.0:\n",
      "   ----------- ----------------------------  4/14 [multidict]\n",
      "      Successfully uninstalled fsspec-2025.12.0\n",
      "   ----------- ----------------------------  4/14 [multidict]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   -------------- -------------------------  5/14 [fsspec]\n",
      "   ----------------- ----------------------  6/14 [frozenlist]\n",
      "   -------------------- -------------------  7/14 [dill]\n",
      "   -------------------- -------------------  7/14 [dill]\n",
      "   -------------------- -------------------  7/14 [dill]\n",
      "   -------------------- -------------------  7/14 [dill]\n",
      "   -------------------- -------------------  7/14 [dill]\n",
      "   -------------------- -------------------  7/14 [dill]\n",
      "   -------------------- -------------------  7/14 [dill]\n",
      "   ---------------------- -----------------  8/14 [aiohappyeyeballs]\n",
      "   ------------------------- --------------  9/14 [yarl]\n",
      "   ---------------------------- ----------- 10/14 [multiprocess]\n",
      "   ---------------------------- ----------- 10/14 [multiprocess]\n",
      "   ---------------------------- ----------- 10/14 [multiprocess]\n",
      "   ---------------------------- ----------- 10/14 [multiprocess]\n",
      "   ---------------------------- ----------- 10/14 [multiprocess]\n",
      "   ---------------------------- ----------- 10/14 [multiprocess]\n",
      "   ---------------------------- ----------- 10/14 [multiprocess]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ---------------------------------- ----- 12/14 [aiohttp]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ------------------------------------- -- 13/14 [datasets]\n",
      "   ---------------------------------------- 14/14 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.2 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 sentencepiece-0.2.1 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch nltk spacy sentencepiece\n",
    "!pip install python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af83479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['newspaper_name', 'published_date\\n', 'headline', 'article_text',\n",
      "       'human_summary', 'news_category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel\n",
    "df = pd.read_excel(\"NewsSumm Dataset.xlsx\")\n",
    "\n",
    "# Inspect columns (VERY IMPORTANT)\n",
    "print(df.columns)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"newssuumm.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b07a6bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Tokenizing articles into sentences (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 348726/348726 [00:52<00:00, 6652.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['newspaper_name', 'published_date\\n', 'headline', 'article_text', 'human_summary', 'news_category', 'sentences'])\n",
      "['Prime Minister Narendra Modi Monday hailed the contribution of medical workers during the coronavirus pandemic.', 'Addressing an event at the Rajiv Gandhi Health University in Bengaluru via video conference, Modi said, â€œThe virus may be an invisible enemy.', 'But our warriors, medical workers are invincible.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# 1ï¸âƒ£ Load dataset\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": \"newssuumm.csv\"}\n",
    ")\n",
    "\n",
    "# 2ï¸âƒ£ Filter bad rows\n",
    "dataset = dataset.filter(\n",
    "    lambda x: x[\"article_text\"] is not None\n",
    "    and isinstance(x[\"article_text\"], str)\n",
    "    and len(x[\"article_text\"].strip()) > 0\n",
    ")\n",
    "\n",
    "# 3ï¸âƒ£ Preprocess function (IMPORT INSIDE FUNCTION âœ…)\n",
    "def preprocess(example):\n",
    "    from nltk.tokenize import sent_tokenize  # ðŸ”¥ REQUIRED for num_proc\n",
    "\n",
    "    text = example[\"article_text\"]\n",
    "\n",
    "    if text is None or not isinstance(text, str) or text.strip() == \"\":\n",
    "        example[\"sentences\"] = []\n",
    "    else:\n",
    "        example[\"sentences\"] = sent_tokenize(text)\n",
    "\n",
    "    return example\n",
    "\n",
    "# 4ï¸âƒ£ Parallel mapping\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    num_proc=4,   # reduce to 2 if needed\n",
    "    desc=\"Tokenizing articles into sentences\"\n",
    ")\n",
    "\n",
    "# 5ï¸âƒ£ Verify\n",
    "print(dataset[\"train\"][0].keys())\n",
    "print(dataset[\"train\"][0][\"sentences\"][:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404f4c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    PegasusTokenizer,\n",
    "    PegasusForConditionalGeneration,\n",
    "    pipeline\n",
    ")\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8413a91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--google--pegasus-cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "SUM_MODEL = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "sum_tokenizer = PegasusTokenizer.from_pretrained(SUM_MODEL)\n",
    "sum_model = PegasusForConditionalGeneration.from_pretrained(SUM_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "073290eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "nli_pipeline = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"roberta-large-mnli\",\n",
    "    framework=\"pt\",  # ðŸ”¥ FORCE PYTORCH\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0979d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article, max_length=128):\n",
    "    inputs = sum_tokenizer(\n",
    "        article,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = sum_model.generate(\n",
    "            **inputs,\n",
    "            num_beams=4,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    return sum_tokenizer.decode(\n",
    "        summary_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78176dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_claim_against_article(claim, article_sentences, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - supported (True / False)\n",
    "    - best_support_sentence\n",
    "    - entailment_score\n",
    "    \"\"\"\n",
    "    best_score = 0.0\n",
    "    best_sentence = None\n",
    "\n",
    "    for sent in article_sentences:\n",
    "        result = nli_pipeline({\n",
    "            \"text\": sent,\n",
    "            \"text_pair\": claim\n",
    "        })\n",
    "\n",
    "        # ðŸ”¥ Handle both dict and list outputs safely\n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "\n",
    "        label = result[\"label\"]\n",
    "        score = result[\"score\"]\n",
    "\n",
    "        if label == \"ENTAILMENT\" and score > best_score:\n",
    "            best_score = score\n",
    "            best_sentence = sent\n",
    "\n",
    "    if best_score >= threshold:\n",
    "        return True, best_sentence, best_score\n",
    "    else:\n",
    "        return False, None, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a22e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_analyzer(article, summary):\n",
    "    article_sentences = sent_tokenize(article)\n",
    "    summary_sentences = sent_tokenize(summary)\n",
    "\n",
    "    analysis = []\n",
    "\n",
    "    for claim in summary_sentences:\n",
    "        supported, support_sentence, score = check_claim_against_article(\n",
    "            claim, article_sentences\n",
    "        )\n",
    "\n",
    "        analysis.append({\n",
    "            \"claim\": claim,\n",
    "            \"supported\": supported,\n",
    "            \"supporting_sentence\": support_sentence,\n",
    "            \"entailment_score\": round(score, 3)\n",
    "        })\n",
    "\n",
    "    return analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d35abfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_risk_level(analysis):\n",
    "    supported = sum(1 for a in analysis if a[\"supported\"])\n",
    "    total = len(analysis)\n",
    "\n",
    "    ratio = supported / total if total > 0 else 0\n",
    "\n",
    "    if ratio >= 0.85:\n",
    "        return \"Low\"\n",
    "    elif ratio >= 0.6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bba86d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY:\n",
      " Prime Minister Narendra Modi Monday hailed the contribution of medical workers during the coronavirus pandemic .<n>He also said that the world is looking at them with gratitude, hope, and seeks both â€˜careâ€™ and â€˜cureâ€™<n>During the address, PM Modi also stressed that violence against COVID warriors will not be tolerated .\n",
      "\n",
      "HALLUCINATION RISK: High\n",
      "{'claim': 'Prime Minister Narendra Modi Monday hailed the contribution of medical workers during the coronavirus pandemic .<n>He also said that the world is looking at them with gratitude, hope, and seeks both â€˜careâ€™ and â€˜cureâ€™<n>During the address, PM Modi also stressed that violence against COVID warriors will not be tolerated .', 'supported': False, 'supporting_sentence': None, 'entailment_score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "article = dataset[\"train\"][0][\"article_text\"]\n",
    "\n",
    "summary = generate_summary(article)\n",
    "analysis = hallucination_analyzer(article, summary)\n",
    "risk = hallucination_risk_level(analysis)\n",
    "\n",
    "print(\"SUMMARY:\\n\", summary)\n",
    "print(\"\\nHALLUCINATION RISK:\", risk)\n",
    "\n",
    "for a in analysis:\n",
    "    print(a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
